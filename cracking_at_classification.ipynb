{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  EDA: Current project 32 unique users. Plot out Distribution of number tweets for each user. \n",
    "  \n",
    "  Bar plot for distribution of number tweets per user.\n",
    "\n",
    "Height of bar should be number of tweets they have.\n",
    " \n",
    " Some users have less than 100 tweets, drop these users.Drop less than 10k users. \n",
    " \n",
    " Further look at userid, and tweet text. Look at text part and then figure process(bag of \n",
    " \n",
    " words)\n",
    " \n",
    " Use this as X. Use this in train test split.\n",
    " \n",
    " Try first naive bayes model in skilearn for labels. Then print out metrics.\n",
    " \n",
    " after:\n",
    " Decision Tree simple(default parameters)\n",
    " \n",
    " Gausian Bayes:\n",
    " \n",
    " Look into what it actually does\n",
    " \n",
    " then calculate accuracy score.\n",
    " \n",
    " Use cross validation. Five folds. \n",
    " \n",
    " If you need go back to lecture(solution in solution sheet for how to do all this).\n",
    " \n",
    " Don't use max features\n",
    " \n",
    " Look into functions. Understand what expected output should be.\n",
    " \n",
    " Learn Acronyms.\n",
    "  \n",
    "  In guide sent by flora, replace count vectorizer with tfidf. \n",
    "  \n",
    "  if i have time, look at Naive Bay. Multiclass?\n",
    "  \n",
    "  y = must be string or object\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "Go back to NLP week and try and do assignment. Individual and pair assignment and compare to \n",
    "\n",
    "solution.  \n",
    "  \n",
    "  Floras last words:\n",
    "  \n",
    "  Be patient and go deep. Make sure each step, \"what am I doing?\"\n",
    "  \n",
    "  Always check. 'Wait a second let me check.'\n",
    "  \n",
    "  Use weekend pace yourself. Back up what you say. If you say you understand the topic \n",
    "  \n",
    "  modeling understand everything. If you say you understand tfidf no the acronym. \n",
    "  \n",
    "  Go back to lecture and really own it. \"If asked to give lecture on Bayes, be able to, own \n",
    "  \n",
    "  it\".\n",
    "  \n",
    "  Saturday: 8pm. show what file worked on. \n",
    "  \n",
    "  Flora: \"why are they taking this step, why this assumption, purpose of this step?\"\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from tweetokenize import Tokenizer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize as wt \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#spell correction\n",
    "from autocorrect import spell\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "from nltk.util import skipgrams\n",
    "from itertools import chain\n",
    "from scipy.cluster import hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/alessandro/Downloads/venezuela_201906_1_tweets_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting userids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.userid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.user_screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking out retweets and non English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mask = df['is_retweet'] == False\n",
    "tweets = df[tweets_mask]\n",
    "\n",
    "tweets_english = tweets[tweets['tweet_language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Users with less than 10,000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets_english[['user_screen_name','tweet_text']]\n",
    "data =data[data.user_screen_name != 'CarolineWB_']\n",
    "data =data[data.user_screen_name != '55DeTN4VJIeKfM9Atr0sSTLomZbJyUWuBNdDK2an1nE=']\n",
    "data =data[data.user_screen_name != 'TDgi60XrT+ylS+rVJEMhb4Y2qzW2HnZmlijAyHNqavc=']\n",
    "data =data[data.user_screen_name != '944Ry+vVZhaSln1T9ctgWQ6N5g45ReoWpWSXfrgKFc=']\n",
    "data =data[data.user_screen_name != 'UaoSsTUDoR7SIA0dvPLYLRt70LG0VUSS3AcrE9FUE=']\n",
    "data =data[data.user_screen_name != 'MUqZv6hxFW92V7lxJyf35c8BU8esdxS6IoV1QGiwwtQ=']\n",
    "data =data[data.user_screen_name != 'TRUMPTRAIN_17']\n",
    "data =data[data.user_screen_name != '_trumpnews_']\n",
    "data =data[data.user_screen_name != 'EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=']\n",
    "data =data[data.user_screen_name != 'AnnabelleBakerF']\n",
    "data =data[data.user_screen_name != 'BreakingNewsDJT']\n",
    "data =data[data.user_screen_name != 'Citizens4Trump_']\n",
    "data =data[data.user_screen_name != 'NoticiasViralTV']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.user_screen_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nighter = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = sns.countplot(x='user_screen_name', data=data)\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "plt.ylabel('Tweets Total', fontsize = 14)\n",
    "plt.xlabel('User Screen Name', fontsize = 14)\n",
    "plt.title('Tweets by user', fontsize = 14)\n",
    "plt.grid(c = 'lightcyan')\n",
    "\n",
    "# import seaborn as sns\n",
    "# planets = sns.load_dataset(\"planets\")\n",
    "# g = sns.factorplot(\"year\", data=planets, aspect=1.5, kind=\"count\", color=\"b\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "\n",
    "\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_toy = data[:500]\n",
    "data_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Tweets to do Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# data1 = []\n",
    "\n",
    "# for i in range(data_toy.shape[0]):\n",
    "#     tweet = data.iloc[i, 1]\n",
    "\n",
    "#     # remove non alphabatic characters\n",
    "#     tweet = re.sub('[^A-Za-z]', ' ', tweet)\n",
    "\n",
    "#     # make words lowercase, because Go and go will be considered as two words\n",
    "#     tweet = tweet.lower()\n",
    "\n",
    "#     # tokenising\n",
    "#     tokenized_tweet = wt(tweet)\n",
    "\n",
    "#     # remove stop words and stemming\n",
    " \n",
    "#     tweet_processed = []\n",
    "#     for word in tokenized_tweet:\n",
    "#         if word not in set(stopwords.words('english')):\n",
    "#             tweet_processed.append(spell(stemmer.stem(word)))\n",
    "\n",
    "#     tweet_text = \" \".join(tweet_processed)\n",
    "#     data1.append(tweet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function to clean_up the twitter_text\n",
    "#### ALL WORKS!!!!\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Additional\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# In this edit I didn't remove # and @ hoping to find the way to remove it together with the followings\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    INPUT: str\n",
    "    OUTPUT: str w/ emojies, urls, hashtags and mentions removed\n",
    "    '''\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "    clean_text = p.clean(text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def remove_symbols(word, symbol_set):\n",
    "    \n",
    "    '''\n",
    "    Removing symbols from word\n",
    "    '''\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)\n",
    "def clean_tweet_text(text_column):\n",
    "    '''\n",
    "    takes a columns in dataframe with tweets text: \n",
    "    Outputs: PD Series of tokenized docs\n",
    "    lower case, \n",
    "    symbol_set charachters removed\n",
    "    punctuation removed\n",
    "    words stemmed and lemmatized\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # converting from pd to list\n",
    "    corpus = text_column.values.tolist()\n",
    "    \n",
    "    #Removing all HTTPs\n",
    "    docs_no_http = [ re.sub(r'https?:\\/\\/.*\\/\\w*', '', doc) for doc in corpus ]\n",
    "    #First ---> tokenize docs\n",
    "    tokenized_docs = [doc.split() for doc in docs_no_http]\n",
    "    # Lower case words in doc\n",
    "    tokenized_docs_lowered  = [[word.lower() for word in doc]\n",
    "                                for doc in tokenized_docs]\n",
    "\n",
    "    # Removing punctuation from docs\n",
    "    cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                    for doc in tokenized_docs_lowered]\n",
    "\n",
    "    ### Removing stop words\n",
    "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#     docs_no_stops = [[word for word in doc if word not in stop_words] \n",
    "#                      for doc in cleaned_docs]\n",
    "    # Lemmatize words in docs\n",
    "    docs_lemmatized = [[lemmer.lemmatize(word) for word in doc]\n",
    "                      for doc in docs_no_stops]\n",
    "    \n",
    "    # Stem words in docs\n",
    "    docs_stemmed = [[stemmer.stem(word) for word in doc]\n",
    "                      for doc in docs_lemmatized]\n",
    "    \n",
    "    # Removes mentions, emotions, hashtags and emojies\n",
    "    docs_no_mentions = [preprocessing_text(' '.join(doc)) for doc in docs_stemmed]\n",
    "    \n",
    "    bag = []\n",
    "    for doc in docs_no_mentions:\n",
    "        if len(doc) >= 2:\n",
    "            bag.append(doc)\n",
    "    \n",
    "    # converts into list of lists\n",
    "    bow = [list(tweet.split(' ')) for tweet in bag]\n",
    "    \n",
    "    \n",
    "    # convert docs into pd series\n",
    "    full_service_docs_series = pd.Series( (v[0] for v in bow) )\n",
    "    \n",
    "    return bag, bow, docs_stemmed, full_service_docs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_bag_english, tweet_full_bow_english, tweet_full_docs_stemmed_english, tweet_english_full_service_doc_series = clean_tweet_text(data['tweet_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_full_bag_english[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(word, symbol_set):\n",
    "    \n",
    "    '''\n",
    "    Removing symbols from word\n",
    "    '''\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: x.lower())\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: remove_symbols(x, punct))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: lemmer.lemmatize(x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_toy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('user_screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "#not necessarily top words\n",
    "#you get top words after you fit it, and get it there. This stage is dimensionality reduction\n",
    "matrix = CountVectorizer(max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> vectorizer = TfidfVectorizer()\n",
    "# >>> X = vectorizer.fit_transform(corpus)\n",
    "# >>> print(vectorizer.get_feature_names())\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# >>> print(X.shape)\n",
    "# (4, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix.fit_transform(data.tweet_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(data.tweet_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floras suggestion for handling this from here\n",
    "1) try TfidfVectorizer instead of CountVectorizer. Think why?\n",
    "2) limit to first predict the top two users, i.e., further reduce your dataset to the two users with the most tweets; and see about the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Braulin way \n",
    "Because this is an unsupervised learning problem, I needed some way to validate the accuracy of my model. To do this, I took a user's entire comment history and randomly pulled out half of their comments, creating a new pseudo-user with these comments. Then I measured my model's success in being able to correctly match this pseudo-user back to the original user those comments were pulled from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions that I'll rework for my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Filtered DF?\n",
    "# The FilterDF is data with authors that have more than 400 comments. Groupby author with count of body.\n",
    "\n",
    "# Split users into users and pseudo-users to compare them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need help understanding what is going on here.\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate comments into corpora of each user's entire comment history\n",
    "comments1 = df1.groupBy(\"author\").agg(F.collect_list(\"body\"))\n",
    "join_comments_udf = udf(lambda x: ' '.join(x), StringType())\n",
    "df1_join_comments = comments1.withColumn(\n",
    "    'corpus', join_comments_udf(comments1['collect_list(body)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    s = s.lower()\n",
    "    token = TweetTokenizer()\n",
    "    return token.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(s):\n",
    "    return [i[1] for i in nltk.pos_tag(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_grams(s):\n",
    "    grams = []\n",
    "    for i in skipgrams(s, 2, 2):\n",
    "        grams.append(str(i))\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file containing the most common skip grams I had previously found from analyzing a previous sample\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skip_grams.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    com_skips = list(reader)\n",
    "\n",
    "skips = com_skips[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Braulin Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my analysis, 150 of some of the most commonly used function words were used to identify user writing styles by the Delta method. The frequencies of each function word were recorded and then standardized by subtracting the mean and dividing by the standard deviation, giving each feature's value the representation of a z-score. The result is a 150-dimensional vector that is positive in a feature dimension where the author uses a word more frequently than the average user, and negative where it is used less than average. The vector of a pseudo-user can then be compared to that of each user most accurately by measure of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest. gradient boosting classifier, partial dependency plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.user_screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a pseudo user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evelyn_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = data\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'OliviaAllenC']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'LaurenJonesGOP_']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'SophiaMillerC']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'SamanthaClarkH']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'Sophia4Trump']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'AlyssaNelsonR']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'LaurenJonesGOP']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'EmmaTurnerBN']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'America4Trump_']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'Laureen4Trump']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'DTrumpTrain_']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'GODBLESSAMERIC']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'TrumpTrainNewss']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'AbbyMartinM']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'CarolineWalkerB']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'AriaWilsonGOP']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'TrumpDailyNewss']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'TrumpNewsDaily_']\n",
    "filtered_df =filtered_df[filtered_df.user_screen_name != 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making actual Pseudo Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.user_screen_name.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The Braulin way\n",
    "# comments1 = df1.groupBy(\"author\").agg(F.collect_list(\"body\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pseudo User Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['tweet_text'] = filtered_df['tweet_text'].apply(lambda x: x.lower())\n",
    "# filtered_df['tweet_text'] = filtered_df['tweet_text'].apply(lambda x: remove_symbols(x, punct))\n",
    "filtered_df['tweet_text'] = filtered_df['tweet_text'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "filtered_df['tweet_text'] = filtered_df['tweet_text'].apply(lambda x: lemmer.lemmatize(x))\n",
    "filtered_df['tweet_text'] = filtered_df['tweet_text'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo, Original, Pseudo_y, Orignal_y = train_test_split(filtered_df.tweet_text, filtered_df.user_screen_name, test_size=.5)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Pseudo))\n",
    "print(len(Original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(max_features=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo_y = [('Fake_Evelyn') for x in Pseudo_y]\n",
    "len(Pseudo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo_y = pd.Series(Pseudo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Pseudo_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The way I'm doing my split of the the user with with most tweets is to split between two users as as series (no labels) is the correct way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df1 = matrix.fit_transform(Pseudo) # X =train = pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2 = matrix.fit_transform(Original) #X test = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes \n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# classifier = GaussianNB()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predict Class\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Accuracy \n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_df1.todense(), Pseudo_y)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_df2.todense())\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(Orignal_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes \n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# classifier = GaussianNB()\n",
    "# classifier.fit(X_df1.todense(), Pseudo_y)\n",
    "\n",
    "# # Predict Class\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Accuracy \n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(y_test, Pseudo_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare pseudo entire corpus including original.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my analysis, 150 of some of the most commonly used function words were used to identify user writing styles by the Delta method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the very least a word cloud. \n",
    "# do you wanna go with pseudo users.\n",
    "pseudo user, countvect, cosine similarity on 150 top standardized words. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations \n",
    "For each users, corpus and frequency of words used.\n",
    "some type of frequency visualization.\n",
    "\n",
    "take out stop words for frequency visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns Braulin made for MCC (by concantenating all these dfs together):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a Df with A column with all comments joined.\n",
    "2. a function that counts links, creates a df that counts links. concantenates it with last df.\n",
    "3.builds a function that drops links, keeps column of counted links (len).\n",
    "4.Does the same with bold formatting (two functions, count and drop bold formating, column of bold formatting remains)\n",
    "5. Same with italics(two functions, count  and drop, keeps column)\n",
    "6.has one that counts 'blocks', which appears to just be white space. drops the space. keeps column of count.\n",
    "7. Does the same for headlines. \n",
    "8. does the same for newlines.\n",
    "9. Only after all this does he tokenize.\n",
    "8. Then he drops punctuation.\n",
    "9.the he has a function that counts words. Has a lambda function that counts length of words. has a plot df that is ordered by total words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCC continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MCCC1 = total_words_df1.select('author', 'word_lengths', 'total_words')\n",
    "2. Then he iterates through this and creates a dictionary and empty list to make plots using word frequency.\n",
    "# RMSE\n",
    "1. then creates word RMSE dictionary\n",
    "# Plots using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First he makes sure all users have at least 400 values saves them in DF named Filter DF.\n",
    "2. He then splits this filtered df in two. Df1, and Df2. Aka original and pseudo.\n",
    "# Verify whether in fact he is combining all respective comments in data frames into One Corpus\n",
    "3. For each respective DF he combines all tweets into one corpus column.\n",
    "4. Creates count links column by the same process. 2 functions.\n",
    "5. Then he tokenizes. Creates (a function for this).\n",
    "6. Then he defines a part of speech tagger function. Calls it on Token column and creates new column Called 'POS'.\n",
    "7. Then he defines a skip gram function. He applies skip gram function on 'POS' column and creates new column.\n",
    "8. Then defines a Skip grams filter function. \n",
    "9. Then he Filters through each user's POS skip-grams and keep them if they are in the most commonly found skip-gram by defining a Skip grams filter function. Applies this to skip gram column. creates new column.\n",
    "10.  Create stop words feature list and add extra features. (makes a column by calling his function on tokens.\n",
    "11. Then he Concatenates each user's list of function words and skip grams to a single array. He builds a function to do this. So each user has an array associated with them containing stop words and skipgrams.\n",
    "12. Then he creates a Count Vectorize the combined function word and skip gram array. For some reason he has 285 words/features. in a column called features, which is a concatenation of stop words and skip grams.\n",
    "13. Then he Normalizes the counts so that they are a percentage of total counts of the features. He does this using some kind of function called Normalizer whos input_col is 'features' and output_col is 'features_norm'. \n",
    "14. Example of last step. tf_norm1 = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1).transform(tf1)\n",
    "15. Then he Standardizes the vector based on average use of each feature among all users. \n",
    "     a. Example code of last step: stdscaler = StandardScaler(inputCol='features_norm', outputCol='scaled', withMean=True)\n",
    "    b. scale_fit1 = stdscaler.fit(tf_norm1)    c. scaled1 = scale_fit1.transform(tf_norm1\n",
    "17. From here he does the same thing for df2.\n",
    "18. After everything is standardized and scaled then he Calculates the cosine similarity for each author in subset 1 against every author in subset 2.\n",
    "How he does that:\n",
    "\n",
    "    sims1 = scaled1.select('author', 'scaled')\n",
    "\n",
    "    sims2 = scaled2.select('author', 'scaled')\n",
    "\n",
    "    similarities = {}:\n",
    "\n",
    "    for i in sims1.rdd.collect():\n",
    "        similarity = {}\n",
    "        auth1, vec1 = i[0], i[1]\n",
    "        for j in sims2.rdd.collect():\n",
    "            auth2, vec2 = j[0], j[1]\n",
    "            cos = vec1.dot(vec2) / (vec2.norm(2)*vec1.norm(2))\n",
    "            similarity[auth2] = cos\n",
    "        similarities[auth1] = similarity\n",
    "19. Then created df of similarities. pdf = pd.DataFrame(similarities)\n",
    "20. Then he splits the cosines of authors who match with the authors who don't match\n",
    "        \n",
    "        cols = pdf.columns\n",
    "        mask = []\n",
    "        for i in pdf:\n",
    "            mask.append(i == pdf.index)\n",
    "        mask = np.array(mask)\n",
    "        mask = mask.T\n",
    "\n",
    "        matches = pdf.values[mask]\n",
    "        non_matches = pdf.values[~mask]\n",
    "21. Then calculates the accuracy of the model:\n",
    "\n",
    "    non_mas = non_matches.reshape(len(matches), -1)\n",
    "    non_mas_max = np.max(non_mas, axis=1)\n",
    "    np.sum(matches > non_mas_max) / len(matches)\n",
    "   \n",
    "22. He then reads matches and non matches from csv.\n",
    "\n",
    "    with open('nonmatches.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        nonma_list = list(reader)\n",
    "\n",
    "    with open('matches.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        match_list = list(reader)\n",
    "\n",
    "    match_list = [[float(x) for x in i] for i in match_list]\n",
    "    match_list = match_list[0]\n",
    "\n",
    "    nonma_list = [[float(x) for x in i] for i in nonma_list]\n",
    "    nonma_list = nonma_list[0]\n",
    "    \n",
    "23. Then he calculates cosine threshold and power for a given alpha level:\n",
    "\n",
    "    n = norm.ppf(0.9999) * np.std(nonma_list) - np.mean(nonma_list)\n",
    "\n",
    "    1 - norm.cdf(n, np.mean(match_list), np.std(match_list))\n",
    "    \n",
    "24. Then he plots:\n",
    "    # Dendogram\n",
    "    sparkdf = scaled1.select('author', 'scaled')\n",
    "    pandaDF = sparkdf.toPandas()\n",
    "    series = pandaDF['scaled'].apply(lambda x: np.array(x.toArray())).as_matrix().reshape(-1, 1)\n",
    "    features = np.apply_along_axis(lambda x: x[0], 1, series)\n",
    "    df = pd.DataFrame(features, index=pandaDF['author'])\n",
    "\n",
    "    threshold = 0.405\n",
    "    Z = hierarchy.linkage(df, 'single', metric=\"cosine\")\n",
    "    hierarchy.set_link_color_palette(None)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15, 7))\n",
    "    hierarchy.dendrogram(Z, ax=axes, color_threshold=threshold, labels=df.index)\n",
    "    axes.axhline(y=0.405, color='r', linestyle='-', label='threshold')\n",
    "    axes.set_ylabel('1 - Cosine')\n",
    "    axes.set_title('Hierarchical Clustering')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "# Matches and non-matches hist\n",
    "\n",
    "    plt.hist(matches, label='matches')\n",
    "    plt.hist(non_matches, label='non-matches')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.legend()\n",
    "    plt.savefig('match_distro.png')\n",
    "\n",
    "# Probability Density\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    ax.plot(x, norm.pdf(x, np.mean(nonma_list), np.std(nonma_list)), label='non-matches')\n",
    "    ax.fill_between(x, 0, norm.pdf(x, np.mean(nonma_list), np.std(nonma_list)), alpha=0.5)\n",
    "    ax.plot(x, norm.pdf(x, np.mean(match_list), np.std(match_list)), label='matches')\n",
    "    ax.fill_between(x, 0, norm.pdf(x, np.mean(match_list), np.std(match_list)), alpha=0.5)\n",
    "    ax.set_title('Probability Density')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.legend()\n",
    "    plt.savefig('prob_density.png')\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nighter.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_nighter.user_screen_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo, Original, Pseudo_y, Orignal_y = train_test_split(filtered_df.tweet_text, filtered_df.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is a list object of two dfs\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SophiaMillerC = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Df's to do cosine similarity testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(tweet):\n",
    "    tokenized = [ word for word in tweet.split() if not(word.startswith(\"http\")) and not(word.startswith('#'))]\n",
    "    return ' '.join(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bag_of_words'] = data['tweet_text'].apply(get_bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: x.lower())\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: remove_symbols(x, punct))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: lemmer.lemmatize(x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EvelynWhiteGOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evelyn_df = data[data.user_screen_name == 'EvelynWhiteGOP']\n",
    "# Evelyn_df = data[data.user_screen_name == 'TrumpNewsz']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'OliviaAllenC']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'LaurenJonesGOP_']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'SophiaMillerC']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'SamanthaClarkH']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'Sophia4Trump']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'AlyssaNelsonR']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'LaurenJonesGOP']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'EmmaTurnerBN']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'America4Trump_']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'Laureen4Trump']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'DTrumpTrain_']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'GODBLESSAMERIC']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'TrumpTrainNewss']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'AbbyMartinM']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'CarolineWalkerB']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'AriaWilsonGOP']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'TrumpDailyNewss']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'TrumpNewsDaily_']\n",
    "# Evelyn_df =Evelyn_df[Evelyn_df.user_screen_name != 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evelyn_1X, Evelyn_2x, Evelyn_1y, Evelyn_2y = train_test_split(Evelyn_df.bag_of_words, Evelyn_df.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OliviaAllenC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OliviaAllenC = data[data.user_screen_name == 'OliviaAllenC']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'LaurenJonesGOP_']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'SophiaMillerC']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'SamanthaClarkH']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'Sophia4Trump']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'AlyssaNelsonR']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'LaurenJonesGOP']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'EmmaTurnerBN']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'America4Trump_']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'Laureen4Trump']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'DTrumpTrain_']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'GODBLESSAMERIC']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'TrumpTrainNewss']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'AbbyMartinM']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'CarolineWalkerB']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'AriaWilsonGOP']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'TrumpDailyNewss']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'TrumpNewsDaily_']\n",
    "# OliviaAllenC =OliviaAllenC[OliviaAllenC.user_screen_name != 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Olivia1X, Olivia_2x, Olivia_1y, Olivia_2y = train_test_split(OliviaAllenC.bag_of_words, OliviaAllenC.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.LaurenJonesGOP_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaurenJonesGOP = data[data.user_screen_name == 'LaurenJonesGOP_']\n",
    "\n",
    "\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'EvelynWhiteGOP']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'OliviaAllenC']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'SophiaMillerC']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'SamanthaClarkH']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'Sophia4Trump']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'AlyssaNelsonR']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'LaurenJonesGOP']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'EmmaTurnerBN']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'America4Trump_']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'Laureen4Trump']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'DTrumpTrain_']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'GODBLESSAMERIC']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'TrumpTrainNewss']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'AbbyMartinM']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'CarolineWalkerB']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'AriaWilsonGOP']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'TrumpDailyNewss']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'TrumpNewsDaily_']\n",
    "# LaurenJonesGOP =LaurenJonesGOP[LaurenJonesGOP.user_screen_name != 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaurenJones1X,LaurenJones1y, LaurenJones2x, LaurenJones2y = train_test_split(LaurenJonesGOP.bag_of_words, LaurenJonesGOP.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.SophiaMillerC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SophiaMillerC = data[data.user_screen_name == 'SophiaMillerC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SophiaMiller1X, SophiaMiller2x, SophiaMiller1y, SophiaMiller2y = train_test_split(SophiaMillerC.bag_of_words, SophiaMillerC.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SamanthaClarkH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " SamanthaClarkH = data[data.user_screen_name == 'SamanthaClarkH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SamanthaClarkH_1X, SamanthaClarkH_2x, SamanthaClarkH_1y, SamanthaClarkH_2y = train_test_split(SamanthaClarkH.bag_of_words, SamanthaClarkH.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sophia4Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Sophia4Trump = data[data.user_screen_name == 'Sophia4Trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sophia4Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sophia4Trump_1X, Sophia4Trump_2x, Sophia4Trump_1y, Sophia4Trump_2y = train_test_split(Sophia4Trump.bag_of_words, Sophia4Trump.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sophia4Trump_1y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. AlyssaNelsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlyssaNelsonR = data[data.user_screen_name == 'AlyssaNelsonR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AlyssaNelsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlyssaNelsonR_1X, AlyssaNelsonR_2x, AlyssaNelsonR_1y, AlyssaNelsonR_2y = train_test_split(AlyssaNelsonR.bag_of_words, AlyssaNelsonR.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlyssaNelsonR_1X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. LaurenJonesGOP aka LJG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LJG2 = data[data.user_screen_name == 'LaurenJonesGOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LJG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " LJG_1X, LJG_2x, LJG_1y, LJG_2y = train_test_split(LJG2.bag_of_words, LJG2.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. EmmaTurnerBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmmaTurnerBN = data[data.user_screen_name == 'EmmaTurnerBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EmmaTurnerBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " EmmaTurner_1X, EmmaTurner_2x, EmmaTurner_1y, EmmaTurner_2y = train_test_split(EmmaTurnerBN.bag_of_words, EmmaTurnerBN.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmmaTurner_1X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. America4Trump_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "America4Trump_  = data[data.user_screen_name == 'America4Trump_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "America4Trump_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " America4Trump_1X, America4Trump_2x, America4Trump_1y, America4Trump_2y = train_test_split(America4Trump_.bag_of_words, America4Trump_.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "America4Trump_1X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Laureen4Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laureen4Trump = data[data.user_screen_name == 'Laureen4Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laureen4Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Laureen4Trump_1X, Laureen4Trump_2x, Laureen4Trump_1y, Laureen4Trump_2y = train_test_split(Laureen4Trump.bag_of_words, Laureen4Trump.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laureen4Trump_1X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DTrumpTrain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  DTrumpTrain_ = data[data.user_screen_name == 'DTrumpTrain_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " DTrumpTrain_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " DTrumpTrain_1X, DTrumpTrain_2x, DTrumpTrain_1y, DTrumpTrain_2y = train_test_split(DTrumpTrain_.bag_of_words, DTrumpTrain_.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. GODBLESSAMERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GODBLESSAMERIC= data[data.user_screen_name == 'GODBLESSAMERIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GODBLESSAMERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GODBLESSAMERIC_1X, GODBLESSAMERIC_2x, GODBLESSAMERIC_1y, GODBLESSAMERIC_2y = train_test_split(GODBLESSAMERIC.bag_of_words, GODBLESSAMERIC.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. TrumpTrainNewss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpTrainNewss= data[data.user_screen_name == 'TrumpTrainNewss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpTrainNewss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpTrain_1X, TrumpTrain_2x, TrumpTrain_1y, TrumpTrain_2y = train_test_split(TrumpTrainNewss.bag_of_words, TrumpTrainNewss.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. AbbyMartinM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbbyMartinM= data[data.user_screen_name == 'AbbyMartinM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AbbyMartinM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbbyMartin_1X, AbbyMartin_2x, AbbyMartin_1y, AbbyMartin_2y = train_test_split(AbbyMartinM.bag_of_words, AbbyMartinM.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. CarolineWalkerB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarolineWalkerB= data[data.user_screen_name == 'CarolineWalkerB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarolineWalkerB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarolineWalker_1X, CarolineWalker_2x, CarolineWalker_1y, CarolineWalker_2y = train_test_split(CarolineWalkerB.bag_of_words, CarolineWalkerB.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. AriaWilsonGOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AriaWilsonGOP= data[data.user_screen_name == 'AriaWilsonGOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AriaWilsonGOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AriaWilsonGOP_1X, AriaWilsonGOP_2x, AriaWilsonGOP_1y, AriaWilsonGOP_2y = train_test_split(AriaWilsonGOP.bag_of_words, AriaWilsonGOP.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. TrumpDailyNewss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpDailyNewss= data[data.user_screen_name == 'TrumpDailyNewss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpDailyNewss_1X, TrumpDailyNewss_2x, TrumpDailyNewss_1y, TrumpDailyNewss_2y = train_test_split(TrumpDailyNewss.bag_of_words, TrumpDailyNewss.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset = TrumpDailyNewss[:,2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. TrumpNewsDaily_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsDaily_= data[data.user_screen_name == 'TrumpNewsDaily_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrumpNewsDaily_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsDaily_1X, TrumpNewsDaily_2x, TrumpNewsDaily_1y, TrumpNewsDaily_2y = train_test_split(TrumpNewsDaily_.bag_of_words,  TrumpNewsDaily_.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. TrumpNewsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsz= data[data.user_screen_name == 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsz_1X, TrumpNewsz_2x, TrumpNewsz_1y, TrumpNewsz_2y = train_test_split(TrumpNewsz.bag_of_words,  TrumpNewsz.user_screen_name, test_size=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to get them to merge on the numbers.. Left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([s1, s2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([TrumpNewsz_1X, TrumpNewsz_1y,TrumpNewsDaily_1X, TrumpNewsDaily_1y,TrumpDailyNewss_1X, TrumpDailyNewss_1y,AriaWilsonGOP_1X,AriaWilsonGOP_1y, CarolineWalker_1X, CarolineWalker_1y,AbbyMartin_1X, AbbyMartin_1y,TrumpTrain_1X, TrumpTrain_1y,GODBLESSAMERIC_1X,GODBLESSAMERIC_1y,DTrumpTrain_1X, DTrumpTrain_1y,Laureen4Trump_1X , Laureen4Trump_1y,America4Trump_1X, America4Trump_1y,EmmaTurner_1X , EmmaTurner_1y,LJG_1X, LJG_1y,AlyssaNelsonR_1X, AlyssaNelsonR_1y, Sophia4Trump_1X, Sophia4Trump_1y, SamanthaClarkH_1X, SamanthaClarkH_1y, SophiaMiller1X, SophiaMiller1y, LaurenJones1X,LaurenJones1y,Olivia1X, Olivia_1y,Evelyn_1X, Evelyn_1y  ],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging ones to become df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [TrumpNewsz_1X, TrumpNewsz_1y,TrumpNewsDaily_1X, TrumpNewsDaily_1y, TrumpDailyNewss_1X, TrumpDailyNewss_1y, AriaWilsonGOP_1X,AriaWilsonGOP_1y, CarolineWalker_1X, CarolineWalker_1y, AbbyMartin_1X, AbbyMartin_1y, TrumpTrain_1X, TrumpTrain_1y, GODBLESSAMERIC_1X,GODBLESSAMERIC_1y, DTrumpTrain_1X, DTrumpTrain_1y, Laureen4Trump_1X , Laureen4Trump_1y, America4Trump_1X, America4Trump_1y, EmmaTurner_1X , EmmaTurner_1y, LJG_1X, LJG_1y, AlyssaNelsonR_1X, AlyssaNelsonR_1y, Sophia4Trump_1X, Sophia4Trump_1y, SamanthaClarkH_1X, SamanthaClarkH_1y, SophiaMiller1X, SophiaMiller1y, LaurenJones1X, LaurenJones1y, Olivia1X, Olivia_1y,Evelyn_1X, Evelyn_1y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = [TrumpNewsz_1X, TrumpNewsz_1y, TrumpNewsDaily_1X,TrumpNewsDaily_1y,TrumpDailyNewss_1X, TrumpDailyNewss_1y, AriaWilsonGOP_1X, AriaWilsonGOP_1y, CarolineWalker_1X, CarolineWalker_1y,AbbyMartin_1X,AbbyMartin_1y       ]\n",
    "#\n",
    "df1 = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to group by other and merge all worsds by author it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging 2's to become df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Olivia_2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([TrumpNewsz_2x, TrumpNewsz_2y,TrumpNewsDaily_2x, TrumpNewsDaily_2y,TrumpDailyNewss_2x, TrumpDailyNewss_2y,AriaWilsonGOP_2x,AriaWilsonGOP_2y, CarolineWalker_2x, CarolineWalker_2y,AbbyMartin_2x, AbbyMartin_2y,TrumpTrain_2x, TrumpTrain_2y,GODBLESSAMERIC_2x,GODBLESSAMERIC_2y,DTrumpTrain_2x, DTrumpTrain_2y,Laureen4Trump_2x , Laureen4Trump_2y,America4Trump_2x, America4Trump_2y,EmmaTurner_2x , EmmaTurner_2y,LJG_2x, LJG_2y,AlyssaNelsonR_2x, AlyssaNelsonR_2y, Sophia4Trump_2x, Sophia4Trump_2y, SamanthaClarkH_2x, SamanthaClarkH_2y, SophiaMiller2x, SophiaMiller2y, LaurenJones2x,LaurenJones2y,Olivia2x, Olivia_2y,Evelyn_2x, Evelyn_2y  ],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.TrumpNewsz_2x, TrumpNewsz_2y\n",
    "2.TrumpNewsDaily_2x, TrumpNewsDaily_2y\n",
    "3.TrumpDailyNewss_2x, TrumpDailyNewss_2y\n",
    "4. AriaWilsonGOP_2x, AriaWilsonGOP_2y\n",
    "5.CarolineWalker_2x,CarolineWalker_2y\n",
    "6. AbbyMartin_2x, AbbyMartin_2y\n",
    "7.TrumpTrain_2x, TrumpTrain_2y\n",
    "8.GODBLESSAMERIC_2x,GODBLESSAMERIC_2y\n",
    "9.DTrumpTrain_2x, DTrumpTrain_2y\n",
    "10. Laureen4Trump_2x, Laureen4Trump_2y\n",
    "11. America4Trump_2x, America4Trump_2y\n",
    "12. EmmaTurner_2x, EmmaTurner_2y\n",
    "13. LJG_2x, LJG_2y\n",
    "14.AlyssaNelsonR_2x, AlyssaNelsonR_2y\n",
    "15.Sophia4Trump_2x, Sophia4Trump_2y\n",
    "16. SamanthaClarkH_2x, SamanthaClarkH_1y\n",
    "17. SophiaMiller2x, SophiaMiller2y\n",
    "19.Olivia_2x,Olivia_2y\n",
    "18. LaurenJones2x, LaurenJones2y\n",
    "19. Olivia_2x, Olivia_2y\n",
    "20.Evelyn_2x, Evelyn_2y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. TrumpNewsz_1X , TrumpNewsz_1y\n",
    "2.TrumpNewsDaily_1X, TrumpNewsDaily_1y\n",
    "3.TrumpDailyNewss_1X, TrumpDailyNewss_1y\n",
    "4. AriaWilsonGOP_1X,AriaWilsonGOP_1y\n",
    "5. CarolineWalker_1X, CarolineWalker_1y\n",
    "6. AbbyMartin_1X, AbbyMartin_1y\n",
    "7.TrumpTrain_1X, TrumpTrain_1y\n",
    "8.GODBLESSAMERIC_1X,GODBLESSAMERIC_1y\n",
    "9.DTrumpTrain_1X, DTrumpTrain_1y\n",
    "10. Laureen4Trump_1X , Laureen4Trump_1y\n",
    "11. America4Trump_1X, America4Trump_1y\n",
    "12.EmmaTurner_1X , EmmaTurner_1y\n",
    "13. LJG_1X, LJG_1y\n",
    "14.AlyssaNelsonR_1X, AlyssaNelsonR_1y\n",
    "15. Sophia4Trump_1X, Sophia4Trump_1y\n",
    "16. SamanthaClarkH_1X, SamanthaClarkH_1y\n",
    "17.SophiaMiller1X, SophiaMiller1y\n",
    "18.LaurenJones1X,LaurenJones1y\n",
    "19. Olivia1X, Olivia_1y\n",
    "20.Evelyn_1X, Evelyn_1y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary pick two users for tfidf for 2 user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(max_features=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmmaTurner_2x = matrix.fit_transform(data.bag_of_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmmaTurner_1x = matrix.fit_transform(data.bag_of_words).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take column of tweets and turn it into list. that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argsort only works on data frame. will tell you which indexes have highest cosine similarity. top 5 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emma_turner_similarities = cosine_similarity(TrumpNewsDaily_1X, TrumpNewsz_1X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 v all[subset]. gives me list of cosine similarities. should be in list. arg sort take top ones. apply them to subset data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
