{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from tweetokenize import Tokenizer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "venezuela_tweets = pd.read_csv('/Users/alessandro/Downloads/venezuela_201906_1_tweets_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweetid', 'userid', 'user_display_name', 'user_screen_name',\n",
       "       'user_reported_location', 'user_profile_description',\n",
       "       'user_profile_url', 'follower_count', 'following_count',\n",
       "       'account_creation_date', 'account_language', 'tweet_language',\n",
       "       'tweet_text', 'tweet_time', 'tweet_client_name', 'in_reply_to_userid',\n",
       "       'in_reply_to_tweetid', 'quoted_tweet_tweetid', 'is_retweet',\n",
       "       'retweet_userid', 'retweet_tweetid', 'latitude', 'longitude',\n",
       "       'quote_count', 'reply_count', 'like_count', 'retweet_count', 'hashtags',\n",
       "       'urls', 'user_mentions', 'poll_choices'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569455, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568620329                                       43231\n",
       "801485278858854401                              39421\n",
       "801200139155214336                              34912\n",
       "801555575687495681                              33503\n",
       "801463213833547776                              32870\n",
       "751630413769113601                              31657\n",
       "751635009052827648                              31073\n",
       "881675624401833984                              27436\n",
       "761401522798333952                              27209\n",
       "892133369965944833                              26370\n",
       "769380097359024128                              26136\n",
       "768237282562310144                              26020\n",
       "896121971989262336                              25472\n",
       "778257194076831744                              22624\n",
       "741092772233482241                              16400\n",
       "801221525718335488                              12618\n",
       "772513370599747585                              12099\n",
       "801442896041873409                              12025\n",
       "741112500356276225                              11367\n",
       "801544458777063424                              11099\n",
       "801609388779769857                              10026\n",
       "801569072618029056                               9929\n",
       "945770142297939968                               9283\n",
       "933477319615877120                               9239\n",
       "933480574685270017                               9208\n",
       "EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=      4256\n",
       "828668571765133317                               4151\n",
       "MUqZv6hxFW92V7lxJyf35c8BU8esdxS6IoV1QGiwwtQ=     3977\n",
       "UaoSsTUDoR7SIA0dvPLYLRt70LG0VUSS3AcrE9FUE=       3932\n",
       "944Ry+vVZhaSln1T9ctgWQ6N5g45ReoWpWSXfrgKFc=      1607\n",
       "TDgi60XrT+ylS+rVJEMhb4Y2qzW2HnZmlijAyHNqavc=      226\n",
       "55DeTN4VJIeKfM9Atr0sSTLomZbJyUWuBNdDK2an1nE=       75\n",
       "862552763414323201                                  4\n",
       "Name: userid, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.userid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only English Words (so it doesn't crash my notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mask = venezuela_tweets['is_retweet'] == False\n",
    "tweets = venezuela_tweets[tweets_mask]\n",
    "\n",
    "tweets_english = tweets[tweets['tweet_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets_english[['user_screen_name','tweet_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       TRUMPTRAIN_17\n",
       "1                                       Laureen4Trump\n",
       "2                                       Laureen4Trump\n",
       "3                                        EmmaTurnerBN\n",
       "4                                      GODBLESSAMERIC\n",
       "5                                      GODBLESSAMERIC\n",
       "6                                      GODBLESSAMERIC\n",
       "7                                      GODBLESSAMERIC\n",
       "8                                      GODBLESSAMERIC\n",
       "9                                      GODBLESSAMERIC\n",
       "10                                     GODBLESSAMERIC\n",
       "11                                     GODBLESSAMERIC\n",
       "12                                    TrumpNewsDaily_\n",
       "13                                    TrumpNewsDaily_\n",
       "14                                    TrumpNewsDaily_\n",
       "15                                    TrumpNewsDaily_\n",
       "16                                    TrumpNewsDaily_\n",
       "17                                    TrumpNewsDaily_\n",
       "38                                      AriaWilsonGOP\n",
       "39                                      AriaWilsonGOP\n",
       "40                                      AriaWilsonGOP\n",
       "41                                      AriaWilsonGOP\n",
       "42                                      AriaWilsonGOP\n",
       "43                                      AriaWilsonGOP\n",
       "44                                      AriaWilsonGOP\n",
       "45                                       DTrumpTrain_\n",
       "46                                       DTrumpTrain_\n",
       "47                                       DTrumpTrain_\n",
       "48                                       DTrumpTrain_\n",
       "49                                     SamanthaClarkH\n",
       "                             ...                     \n",
       "564118                                TrumpDailyNewss\n",
       "564119                                TrumpTrainNewss\n",
       "564120                                TrumpTrainNewss\n",
       "564121                                TrumpDailyNewss\n",
       "564122                                TrumpDailyNewss\n",
       "564123                                TrumpDailyNewss\n",
       "564124                                TrumpDailyNewss\n",
       "564125                                TrumpDailyNewss\n",
       "564126                                TrumpDailyNewss\n",
       "564127                                TrumpDailyNewss\n",
       "564128                                TrumpDailyNewss\n",
       "564129                                TrumpDailyNewss\n",
       "564130                                TrumpDailyNewss\n",
       "564131                                TrumpDailyNewss\n",
       "564132                                TrumpDailyNewss\n",
       "564133                                 EvelynWhiteGOP\n",
       "564134                                 EvelynWhiteGOP\n",
       "564135                                 EvelynWhiteGOP\n",
       "564136                                 EvelynWhiteGOP\n",
       "564137                                 EvelynWhiteGOP\n",
       "564138                                 EvelynWhiteGOP\n",
       "564139                                 EvelynWhiteGOP\n",
       "564140                                 EvelynWhiteGOP\n",
       "564141                                 EvelynWhiteGOP\n",
       "564142                                 EvelynWhiteGOP\n",
       "564143                                 EvelynWhiteGOP\n",
       "564144                                 EvelynWhiteGOP\n",
       "564145                                 EvelynWhiteGOP\n",
       "564146                                 EvelynWhiteGOP\n",
       "564147    EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=\n",
       "Name: user_screen_name, Length: 494275, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to clean_up the twitter_text\n",
    "#### ALL WORKS!!!!\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Additional\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# In this edit I didn't remove # and @ hoping to find the way to remove it together with the followings\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    INPUT: str\n",
    "    OUTPUT: str w/ emojies, urls, hashtags and mentions removed\n",
    "    '''\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "    clean_text = p.clean(text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def remove_symbols(word, symbol_set):\n",
    "    \n",
    "    '''\n",
    "    Removing symbols from word\n",
    "    '''\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)\n",
    "def clean_tweet_text(text_column):\n",
    "    '''\n",
    "    takes a columns in dataframe with tweets text: \n",
    "    Outputs: PD Series of tokenized docs\n",
    "    lower case, \n",
    "    symbol_set charachters removed\n",
    "    punctuation removed\n",
    "    words stemmed and lemmatized\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # converting from pd to list\n",
    "    corpus = text_column.values.tolist()\n",
    "    \n",
    "    #Removing all HTTPs\n",
    "    docs_no_http = [ re.sub(r'https?:\\/\\/.*\\/\\w*', '', doc) for doc in corpus ]\n",
    "    #First ---> tokenize docs\n",
    "    tokenized_docs = [doc.split() for doc in docs_no_http]\n",
    "    # Lower case words in doc\n",
    "    tokenized_docs_lowered  = [[word.lower() for word in doc]\n",
    "                                for doc in tokenized_docs]\n",
    "\n",
    "    # Removing punctuation from docs\n",
    "    cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                    for doc in tokenized_docs_lowered]\n",
    "\n",
    "    ### Removing stop words\n",
    "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#     docs_no_stops = [[word for word in doc if word not in stop_words] \n",
    "#                      for doc in cleaned_docs]\n",
    "    # Lemmatize words in docs\n",
    "    docs_lemmatized = [[lemmer.lemmatize(word) for word in doc]\n",
    "                      for doc in docs_no_stops]\n",
    "    \n",
    "    # Stem words in docs\n",
    "    docs_stemmed = [[stemmer.stem(word) for word in doc]\n",
    "                      for doc in docs_lemmatized]\n",
    "    \n",
    "    # Removes mentions, emotions, hashtags and emojies\n",
    "    docs_no_mentions = [preprocessing_text(' '.join(doc)) for doc in docs_stemmed]\n",
    "    \n",
    "    bag = []\n",
    "    for doc in docs_no_mentions:\n",
    "        if len(doc) >= 2:\n",
    "            bag.append(doc)\n",
    "    \n",
    "    # converts into list of lists\n",
    "    bow = [list(tweet.split(' ')) for tweet in bag]\n",
    "    \n",
    "    \n",
    "    # convert docs into pd series\n",
    "    full_service_docs_series = pd.Series( (v[0] for v in bow) )\n",
    "    \n",
    "    return bag, bow, docs_stemmed, full_service_docs_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus and Users built to loop through. Corpus will be X and Users Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(tweet):\n",
    "    tokenized = [ word for word in tweet.split() if not(word.startswith(\"http\")) and not(word.startswith('#'))]\n",
    "    return ' '.join(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function to clean_up the twitter_text\n",
    "#### ALL WORKS!!!!\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Additional\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# In this edit I didn't remove # and @ hoping to find the way to remove it together with the followings\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    INPUT: str\n",
    "    OUTPUT: str w/ emojies, urls, hashtags and mentions removed\n",
    "    '''\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "    clean_text = p.clean(text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def remove_symbols(word, symbol_set):\n",
    "    \n",
    "    '''\n",
    "    Removing symbols from word\n",
    "    '''\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)\n",
    "def clean_tweet_text(text_column):\n",
    "    '''\n",
    "    takes a columns in dataframe with tweets text: \n",
    "    Outputs: PD Series of tokenized docs\n",
    "    lower case, \n",
    "    symbol_set charachters removed\n",
    "    punctuation removed\n",
    "    words stemmed and lemmatized\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # converting from pd to list\n",
    "    corpus = text_column.values.tolist()\n",
    "    \n",
    "    #Removing all HTTPs\n",
    "    docs_no_http = [ re.sub(r'https?:\\/\\/.*\\/\\w*', '', doc) for doc in corpus ]\n",
    "    #First ---> tokenize docs\n",
    "    tokenized_docs = [doc.split() for doc in docs_no_http]\n",
    "    # Lower case words in doc\n",
    "    tokenized_docs_lowered  = [[word.lower() for word in doc]\n",
    "                                for doc in tokenized_docs]\n",
    "\n",
    "    # Removing punctuation from docs\n",
    "    cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                    for doc in tokenized_docs_lowered]\n",
    "\n",
    "    ### Removing stop words\n",
    "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#     docs_no_stops = [[word for word in doc if word not in stop_words] \n",
    "#                      for doc in cleaned_docs]\n",
    "    # Lemmatize words in docs\n",
    "    docs_lemmatized = [[lemmer.lemmatize(word) for word in doc]\n",
    "                      for doc in docs_no_stops]\n",
    "    \n",
    "    # Stem words in docs\n",
    "    docs_stemmed = [[stemmer.stem(word) for word in doc]\n",
    "                      for doc in docs_lemmatized]\n",
    "    \n",
    "    # Removes mentions, emotions, hashtags and emojies\n",
    "    docs_no_mentions = [preprocessing_text(' '.join(doc)) for doc in docs_stemmed]\n",
    "    \n",
    "    bag = []\n",
    "    for doc in docs_no_mentions:\n",
    "        if len(doc) >= 2:\n",
    "            bag.append(doc)\n",
    "    \n",
    "    # converts into list of lists\n",
    "    bow = [list(tweet.split(' ')) for tweet in bag]\n",
    "    \n",
    "    \n",
    "    # convert docs into pd series\n",
    "    full_service_docs_series = pd.Series( (v[0] for v in bow) )\n",
    "    \n",
    "    return bag, bow, docs_stemmed, full_service_docs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['bag_of_words'] = data['tweet_text'].apply(get_bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRUMPTRAIN_17</td>\n",
       "      <td>Truth About “Right to Healthcare” Every Lib Ne...</td>\n",
       "      <td>truth about “right to healthcare” every lib ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laureen4Trump</td>\n",
       "      <td>BREAKING: Air Force Makes Tragic Announcement,...</td>\n",
       "      <td>breaking air force makes tragic announcement p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laureen4Trump</td>\n",
       "      <td>HAPPENING NOW: Disney Facing a Scandal So Cata...</td>\n",
       "      <td>happening now disney facing a scandal so catas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EmmaTurnerBN</td>\n",
       "      <td>Another Stupid Gun Control Cartoon From the Le...</td>\n",
       "      <td>another stupid gun control cartoon from the left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GODBLESSAMERIC</td>\n",
       "      <td>➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...</td>\n",
       "      <td>➡️ 90 pounds of cocaine found on ship owned by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name                                         tweet_text  \\\n",
       "0    TRUMPTRAIN_17  Truth About “Right to Healthcare” Every Lib Ne...   \n",
       "1    Laureen4Trump  BREAKING: Air Force Makes Tragic Announcement,...   \n",
       "2    Laureen4Trump  HAPPENING NOW: Disney Facing a Scandal So Cata...   \n",
       "3     EmmaTurnerBN  Another Stupid Gun Control Cartoon From the Le...   \n",
       "4   GODBLESSAMERIC  ➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...   \n",
       "\n",
       "                                        bag_of_words  \n",
       "0  truth about “right to healthcare” every lib ne...  \n",
       "1  breaking air force makes tragic announcement p...  \n",
       "2  happening now disney facing a scandal so catas...  \n",
       "3   another stupid gun control cartoon from the left  \n",
       "4  ➡️ 90 pounds of cocaine found on ship owned by...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: x.lower())\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: remove_symbols(x, punct))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: lemmer.lemmatize(x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Df's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Evelyn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evelyn_df = data[data.user_screen_name == 'EvelynWhiteGOP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 OliviaAllenC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OliviaAllenC = data[data.user_screen_name == 'OliviaAllenC']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrumpNewz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsz= data[data.user_screen_name == 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing TrumpNewz to OliviaAllenC (similar size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvelynWhiteGOP    38879\n",
       "TrumpNewsz        34665\n",
       "Name: user_screen_name, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fletchers_home = pd.concat([Evelyn_df, TrumpNewsz])\n",
    "fletchers_home.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66ba86bc92e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_tfidf_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletchers_home\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_screen_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document_tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "X = document_tfidf_matrix\n",
    "y = fletchers_home.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "document_tfidf_matrix = tfidf.fit_transform(fletchers_home.bag_of_words)\n",
    "# print(sorted(tfidf.vocabulary_))\n",
    "# print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first do train test split and then vectorize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = document_tfidf_matrix\n",
    "y = fletchers_home.user_screen_name\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=2,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olivia allen v trumpnews count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(max_features=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix.fit_transform(fletchers_home.bag_of_words).toarray()\n",
    "y = fletchers_home.user_screen_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=2,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier()\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrumpNewsDaily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing Trump news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TrumpNewsDaily_= data[data.user_screen_name == 'TrumpNewsDaily_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrumpNewsz         34665\n",
       "TrumpNewsDaily_    32407\n",
       "Name: user_screen_name, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_news_comparison = pd.concat([TrumpNewsDaily_, TrumpNewsz])\n",
    "trump_news_comparison.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "# document_tfidf_matrix = tfidf.fit_transform(trump_news_comparison.bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trump_news_comparison.bag_of_words\n",
    "y = trump_news_comparison.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "X_train = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<46950x150 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 229131 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=2,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504770897525097"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3cb542f5d412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n\u001b[0m\u001b[1;32m      2\u001b[0m      max_depth=1, random_state=0).fit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try Lauren and Lauren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.LaurenJonesGOP_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear regression, decison tree, latent dirichlet allocation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaurenJonesGOP = data[data.user_screen_name == 'LaurenJonesGOP_']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.SophiaMillerC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SophiaMillerC = data[data.user_screen_name == 'SophiaMillerC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SamanthaClarkH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " SamanthaClarkH = data[data.user_screen_name == 'SamanthaClarkH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sophia4Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Sophia4Trump = data[data.user_screen_name == 'Sophia4Trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. AlyssaNelsonR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(max_features=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrix.fit_transform(data.tweet_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TFIDF vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run count vec instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "document_tfidf_matrix = tfidf.fit_transform(data.bag_of_words)\n",
    "# print(sorted(tfidf.vocabulary_))\n",
    "print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_matrix.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(document_tfidf_matrix.todense(),columns=tfidf.vocabulary_)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = document_tfidf_matrix\n",
    "y = data.user_screen_name\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[:3,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This kernel dies which makes it so  i have to restart everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This kernel dies which makes it so everything i have to restart everything\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search on n_estimators, max_depth :10,200,300 and then decrease max depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn ensemble gradient boost, grid search: max depth, max features, learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words, count vec. \n",
    "# take stop words and look at distribution. count. using that as additional feature. \n",
    "# take original x. append that to your X. \n",
    "# libraries out there to use parts of speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamid Advice: Binary pick two users for tfidf for 2 user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=2,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report, precision_recall_fscore_support, accuracy_score\n",
    "precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_test, y_pred)\n",
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [2,6],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier()\n",
    "\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run it in atom ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "# clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Parameters\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10,100,1000]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn ensemble gradient boost, grid search: max depth, max features, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=3, random_state=0, multi_class='multinomial').fit(X_train, y_train)\n",
    "lr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: x.lower())\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: remove_symbols(x, punct))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: lemmer.lemmatize(x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This kernel dies which makes it so everything i have to restart everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# X = document_tfidf_matrix.toarray()\n",
    "# svd = TruncatedSVD(n_components=3000, random_state=42)\n",
    "# svd.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to lecture read everything.\n",
    "\n",
    "\n",
    "Realistic Braulin\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
