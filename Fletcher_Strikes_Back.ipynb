{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from tweetokenize import Tokenizer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "venezuela_tweets = pd.read_csv('/Users/alessandro/Downloads/venezuela_201906_1_tweets_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweetid', 'userid', 'user_display_name', 'user_screen_name',\n",
       "       'user_reported_location', 'user_profile_description',\n",
       "       'user_profile_url', 'follower_count', 'following_count',\n",
       "       'account_creation_date', 'account_language', 'tweet_language',\n",
       "       'tweet_text', 'tweet_time', 'tweet_client_name', 'in_reply_to_userid',\n",
       "       'in_reply_to_tweetid', 'quoted_tweet_tweetid', 'is_retweet',\n",
       "       'retweet_userid', 'retweet_tweetid', 'latitude', 'longitude',\n",
       "       'quote_count', 'reply_count', 'like_count', 'retweet_count', 'hashtags',\n",
       "       'urls', 'user_mentions', 'poll_choices'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569455, 31)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568620329                                       43231\n",
       "801485278858854401                              39421\n",
       "801200139155214336                              34912\n",
       "801555575687495681                              33503\n",
       "801463213833547776                              32870\n",
       "751630413769113601                              31657\n",
       "751635009052827648                              31073\n",
       "881675624401833984                              27436\n",
       "761401522798333952                              27209\n",
       "892133369965944833                              26370\n",
       "769380097359024128                              26136\n",
       "768237282562310144                              26020\n",
       "896121971989262336                              25472\n",
       "778257194076831744                              22624\n",
       "741092772233482241                              16400\n",
       "801221525718335488                              12618\n",
       "772513370599747585                              12099\n",
       "801442896041873409                              12025\n",
       "741112500356276225                              11367\n",
       "801544458777063424                              11099\n",
       "801609388779769857                              10026\n",
       "801569072618029056                               9929\n",
       "945770142297939968                               9283\n",
       "933477319615877120                               9239\n",
       "933480574685270017                               9208\n",
       "EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=      4256\n",
       "828668571765133317                               4151\n",
       "MUqZv6hxFW92V7lxJyf35c8BU8esdxS6IoV1QGiwwtQ=     3977\n",
       "UaoSsTUDoR7SIA0dvPLYLRt70LG0VUSS3AcrE9FUE=       3932\n",
       "944Ry+vVZhaSln1T9ctgWQ6N5g45ReoWpWSXfrgKFc=      1607\n",
       "TDgi60XrT+ylS+rVJEMhb4Y2qzW2HnZmlijAyHNqavc=      226\n",
       "55DeTN4VJIeKfM9Atr0sSTLomZbJyUWuBNdDK2an1nE=       75\n",
       "862552763414323201                                  4\n",
       "Name: userid, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venezuela_tweets.userid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only English Words (so it doesn't crash my notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mask = venezuela_tweets['is_retweet'] == False\n",
    "tweets = venezuela_tweets[tweets_mask]\n",
    "\n",
    "tweets_english = tweets[tweets['tweet_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets_english[['user_screen_name','tweet_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       TRUMPTRAIN_17\n",
       "1                                       Laureen4Trump\n",
       "2                                       Laureen4Trump\n",
       "3                                        EmmaTurnerBN\n",
       "4                                      GODBLESSAMERIC\n",
       "5                                      GODBLESSAMERIC\n",
       "6                                      GODBLESSAMERIC\n",
       "7                                      GODBLESSAMERIC\n",
       "8                                      GODBLESSAMERIC\n",
       "9                                      GODBLESSAMERIC\n",
       "10                                     GODBLESSAMERIC\n",
       "11                                     GODBLESSAMERIC\n",
       "12                                    TrumpNewsDaily_\n",
       "13                                    TrumpNewsDaily_\n",
       "14                                    TrumpNewsDaily_\n",
       "15                                    TrumpNewsDaily_\n",
       "16                                    TrumpNewsDaily_\n",
       "17                                    TrumpNewsDaily_\n",
       "38                                      AriaWilsonGOP\n",
       "39                                      AriaWilsonGOP\n",
       "40                                      AriaWilsonGOP\n",
       "41                                      AriaWilsonGOP\n",
       "42                                      AriaWilsonGOP\n",
       "43                                      AriaWilsonGOP\n",
       "44                                      AriaWilsonGOP\n",
       "45                                       DTrumpTrain_\n",
       "46                                       DTrumpTrain_\n",
       "47                                       DTrumpTrain_\n",
       "48                                       DTrumpTrain_\n",
       "49                                     SamanthaClarkH\n",
       "                             ...                     \n",
       "564118                                TrumpDailyNewss\n",
       "564119                                TrumpTrainNewss\n",
       "564120                                TrumpTrainNewss\n",
       "564121                                TrumpDailyNewss\n",
       "564122                                TrumpDailyNewss\n",
       "564123                                TrumpDailyNewss\n",
       "564124                                TrumpDailyNewss\n",
       "564125                                TrumpDailyNewss\n",
       "564126                                TrumpDailyNewss\n",
       "564127                                TrumpDailyNewss\n",
       "564128                                TrumpDailyNewss\n",
       "564129                                TrumpDailyNewss\n",
       "564130                                TrumpDailyNewss\n",
       "564131                                TrumpDailyNewss\n",
       "564132                                TrumpDailyNewss\n",
       "564133                                 EvelynWhiteGOP\n",
       "564134                                 EvelynWhiteGOP\n",
       "564135                                 EvelynWhiteGOP\n",
       "564136                                 EvelynWhiteGOP\n",
       "564137                                 EvelynWhiteGOP\n",
       "564138                                 EvelynWhiteGOP\n",
       "564139                                 EvelynWhiteGOP\n",
       "564140                                 EvelynWhiteGOP\n",
       "564141                                 EvelynWhiteGOP\n",
       "564142                                 EvelynWhiteGOP\n",
       "564143                                 EvelynWhiteGOP\n",
       "564144                                 EvelynWhiteGOP\n",
       "564145                                 EvelynWhiteGOP\n",
       "564146                                 EvelynWhiteGOP\n",
       "564147    EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=\n",
       "Name: user_screen_name, Length: 494275, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus and Users built to loop through. Corpus will be X and Users Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(tweet):\n",
    "    tokenized = [ word for word in tweet.split() if not(word.startswith(\"http\")) and not(word.startswith('#'))]\n",
    "    return ' '.join(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bag_of_words(text):\n",
    "#     soup = BeautifulSoup(text, 'html.parser')\n",
    "#     text_list = [p.getText() for p in soup.find_all(\"p\")]\n",
    "#     sent_list = [sent.replace('\\xa0', ' ').lower() for sent in text_list]\n",
    "    \n",
    "#     sent_isalnum = []\n",
    "#     for i in range(len(sent_list)):\n",
    "#         sent_isalnum.append(re.sub(r'[^a-z]+', ' ', sent_list[i]))\n",
    "    \n",
    "#     return ' '.join(sent_isalnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['bag_of_words'] = data['tweet_text'].apply(get_bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRUMPTRAIN_17</td>\n",
       "      <td>Truth About “Right to Healthcare” Every Lib Ne...</td>\n",
       "      <td>Truth About “Right to Healthcare” Every Lib Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laureen4Trump</td>\n",
       "      <td>BREAKING: Air Force Makes Tragic Announcement,...</td>\n",
       "      <td>BREAKING: Air Force Makes Tragic Announcement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laureen4Trump</td>\n",
       "      <td>HAPPENING NOW: Disney Facing a Scandal So Cata...</td>\n",
       "      <td>HAPPENING NOW: Disney Facing a Scandal So Cata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EmmaTurnerBN</td>\n",
       "      <td>Another Stupid Gun Control Cartoon From the Le...</td>\n",
       "      <td>Another Stupid Gun Control Cartoon From the Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GODBLESSAMERIC</td>\n",
       "      <td>➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...</td>\n",
       "      <td>➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name                                         tweet_text  \\\n",
       "0    TRUMPTRAIN_17  Truth About “Right to Healthcare” Every Lib Ne...   \n",
       "1    Laureen4Trump  BREAKING: Air Force Makes Tragic Announcement,...   \n",
       "2    Laureen4Trump  HAPPENING NOW: Disney Facing a Scandal So Cata...   \n",
       "3     EmmaTurnerBN  Another Stupid Gun Control Cartoon From the Le...   \n",
       "4   GODBLESSAMERIC  ➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...   \n",
       "\n",
       "                                        bag_of_words  \n",
       "0  Truth About “Right to Healthcare” Every Lib Ne...  \n",
       "1  BREAKING: Air Force Makes Tragic Announcement,...  \n",
       "2  HAPPENING NOW: Disney Facing a Scandal So Cata...  \n",
       "3   Another Stupid Gun Control Cartoon From the Left  \n",
       "4  ➡️ 90 POUNDS OF COCAINE FOUND ON SHIP OWNED BY...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through Corpus and appending each bag of words in a_tweets (List of Lists)\n",
    "# Looping through Users and appending each user in a_user (List of Lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# corpus = data.bag_of_words\n",
    "# # a_tweets= [corpus[0],corpus[1]]\n",
    "# users = data.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tweets = []\n",
    "# for corp in corpus:\n",
    "#     a_tweets.append(corp)\n",
    "\n",
    "\n",
    "# a_user = []\n",
    "# for user in users:\n",
    "#     a_user.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Tweets Enumerated so I can list comp through them and use string methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# a_tweets_lower = [[word.lower() for word in tweet]\n",
    "#                  for tweet in a_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords_ = set(stopwords.words('english'))\n",
    "# print(\"--- stopwords in english: {}\".format(stopwords_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# punctuation_ = set(string.punctuation)\n",
    "# print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "# def filter_tokens(sent):\n",
    "#     return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "# tweets_filtered = list(map(filter_tokens, a_tweets_lower))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # term occurence = counting distinct words in each bag\n",
    "# term_occ = list(map(lambda tweet : Counter(tweet), tweets_filtered))\n",
    "\n",
    "# # term frequency = occurences over length of bag\n",
    "# term_freq = list()\n",
    "# for i in range(len(a_tweets)):\n",
    "#     term_freq.append( {k: (v / float(len(tweets_filtered[i])))\n",
    "#                        for k, v in term_occ[i].items()} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # document occurence = number of documents having this word\n",
    "# # term frequency = occurences over length of bag\n",
    "\n",
    "# doc_occ = Counter( [word for tweet in tweets_filtered for word in set(tweet)] )\n",
    "\n",
    "# # document frequency = occurences over length of corpus\n",
    "# doc_freq = {k: (v / float(len(tweets_filtered)))\n",
    "#             for k, v in doc_occ.items()}\n",
    "\n",
    "# # displaying vocabulary\n",
    "# # print(\"\\n--- full vocabulary: {}\".format(doc_occ))\n",
    "# # print(\"\\n--- doc freq: {}\".format(doc_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the vocabulary for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "# min_df = 0.0\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "# vocabulary = [ k for k,v in doc_freq.items()]\n",
    "\n",
    "# original line\n",
    "# vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = [ k for k,v in doc_freq.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TFIDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48681391 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.42975196 0.56383248 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=100)\n",
    "document_tfidf_matrix = tfidf.fit_transform(data.bag_of_words)\n",
    "# print(sorted(tfidf.vocabulary_))\n",
    "print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494275, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_matrix.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>right</th>\n",
       "      <th>to</th>\n",
       "      <th>breaking</th>\n",
       "      <th>makes</th>\n",
       "      <th>please</th>\n",
       "      <th>now</th>\n",
       "      <th>that</th>\n",
       "      <th>from</th>\n",
       "      <th>the</th>\n",
       "      <th>...</th>\n",
       "      <th>their</th>\n",
       "      <th>do</th>\n",
       "      <th>down</th>\n",
       "      <th>russia</th>\n",
       "      <th>rt</th>\n",
       "      <th>here</th>\n",
       "      <th>muslim</th>\n",
       "      <th>him</th>\n",
       "      <th>did</th>\n",
       "      <th>an</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.486814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  right   to  breaking  makes  please  now  that      from  the  \\\n",
       "0  0.486814    0.0  0.0       0.0    0.0     0.0  0.0   0.0  0.000000  0.0   \n",
       "1  0.000000    0.0  0.0       0.0    0.0     0.0  0.0   0.0  0.000000  0.0   \n",
       "2  0.000000    0.0  0.0       0.0    0.0     0.0  0.0   0.0  0.000000  0.0   \n",
       "3  0.000000    0.0  0.0       0.0    0.0     0.0  0.0   0.0  0.000000  0.0   \n",
       "4  0.000000    0.0  0.0       0.0    0.0     0.0  0.0   0.0  0.623244  0.0   \n",
       "\n",
       "   ...  their   do  down  russia   rt  here  muslim  him  did   an  \n",
       "0  ...    0.0  0.0   0.0     0.0  0.0   0.0     0.0  0.0  0.0  0.0  \n",
       "1  ...    0.0  0.0   0.0     0.0  0.0   0.0     0.0  0.0  0.0  0.0  \n",
       "2  ...    0.0  0.0   0.0     0.0  0.0   0.0     0.0  0.0  0.0  0.0  \n",
       "3  ...    0.0  0.0   0.0     0.0  0.0   0.0     0.0  0.0  0.0  0.0  \n",
       "4  ...    0.0  0.0   0.0     0.0  0.0   0.0     0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(document_tfidf_matrix.todense(),columns=tfidf.vocabulary_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494275, 100)\n",
      "(494275,)\n"
     ]
    }
   ],
   "source": [
    "X = document_tfidf_matrix\n",
    "y = data.user_screen_name\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.48681391, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This kernel dies which makes it so  i have to restart everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This kernel dies which makes it so everything i have to restart everything\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=2,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08833333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "                                 AbbyMartinM       0.00      0.00      0.00       152\n",
      "                               AlyssaNelsonR       0.00      0.00      0.00        66\n",
      "                              America4Trump_       0.00      0.00      0.00       186\n",
      "                             AnnabelleBakerF       0.00      0.00      0.00        64\n",
      "                               AriaWilsonGOP       0.09      1.00      0.16       265\n",
      "                             BreakingNewsDJT       0.00      0.00      0.00        17\n",
      "                             CarolineWalkerB       0.00      0.00      0.00       227\n",
      "                             Citizens4Trump_       0.00      0.00      0.00        19\n",
      "                                DTrumpTrain_       0.00      0.00      0.00       136\n",
      " EAMY+7SRc0r53i1vfKV4UvVAc+DAI4rtEpUjFSbwbQ=       0.00      0.00      0.00        20\n",
      "                                EmmaTurnerBN       0.00      0.00      0.00       129\n",
      "                              EvelynWhiteGOP       0.00      0.00      0.00       207\n",
      "                              GODBLESSAMERIC       0.00      0.00      0.00       157\n",
      "                               Laureen4Trump       0.00      0.00      0.00       148\n",
      "                              LaurenJonesGOP       0.00      0.00      0.00        95\n",
      "                             LaurenJonesGOP_       0.00      0.00      0.00        64\n",
      "MUqZv6hxFW92V7lxJyf35c8BU8esdxS6IoV1QGiwwtQ=       0.00      0.00      0.00        18\n",
      "                                OliviaAllenC       0.00      0.00      0.00        36\n",
      "                              SamanthaClarkH       0.00      0.00      0.00        87\n",
      "                                Sophia4Trump       0.00      0.00      0.00        65\n",
      "                               SophiaMillerC       0.00      0.00      0.00       104\n",
      "                               TRUMPTRAIN_17       0.00      0.00      0.00        20\n",
      "                             TrumpDailyNewss       0.00      0.00      0.00       189\n",
      "                             TrumpNewsDaily_       0.00      0.00      0.00       171\n",
      "                                  TrumpNewsz       0.00      0.00      0.00       203\n",
      "                             TrumpTrainNewss       0.00      0.00      0.00       129\n",
      "  UaoSsTUDoR7SIA0dvPLYLRt70LG0VUSS3AcrE9FUE=       0.00      0.00      0.00        22\n",
      "                                 _trumpnews_       0.00      0.00      0.00         4\n",
      "\n",
      "                                    accuracy                           0.09      3000\n",
      "                                   macro avg       0.00      0.04      0.01      3000\n",
      "                                weighted avg       0.01      0.09      0.01      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,29,10001) into shape (10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-60f3396445ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2103\u001b[0m             coefs_paths = np.reshape(\n\u001b[1;32m   2104\u001b[0m                 \u001b[0mcoefs_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_ratios_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m             )\n\u001b[1;32m   2107\u001b[0m             \u001b[0;31m# equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    290\u001b[0m            [5, 6]])\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (10,29,10001) into shape (10)"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X_train, y_train)\n",
    "lr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This kernel dies which makes it so everything i have to restart everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# X = document_tfidf_matrix.toarray()\n",
    "# svd = TruncatedSVD(n_components=3000, random_state=42)\n",
    "# svd.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to lecture read everything.\n",
    "\n",
    "\n",
    "Realistic Braulin\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
