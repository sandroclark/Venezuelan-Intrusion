{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# from tweetokenize import Tokenizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function to clean_up the twitter_text\n",
    "#### ALL WORKS!!!!\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Additional\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# In this edit I didn't remove # and @ hoping to find the way to remove it together with the followings\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import preprocessor as p\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    INPUT: str\n",
    "    OUTPUT: str w/ emojies, urls, hashtags and mentions removed\n",
    "    '''\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "    clean_text = p.clean(text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def remove_symbols(word, symbol_set):\n",
    "    \n",
    "    '''\n",
    "    Removing symbols from word\n",
    "    '''\n",
    "    return ''.join(char for char in word \n",
    "                    if char not in symbol_set)\n",
    "def clean_tweet_text(text_column):\n",
    "    '''\n",
    "    takes a columns in dataframe with tweets text: \n",
    "    Outputs: PD Series of tokenized docs\n",
    "    lower case, \n",
    "    symbol_set charachters removed\n",
    "    punctuation removed\n",
    "    words stemmed and lemmatized\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # converting from pd to list\n",
    "    corpus = text_column.values.tolist()\n",
    "    \n",
    "    #Removing all HTTPs\n",
    "    docs_no_http = [ re.sub(r'https?:\\/\\/.*\\/\\w*', '', doc) for doc in corpus ]\n",
    "    #First ---> tokenize docs\n",
    "    tokenized_docs = [doc.split() for doc in docs_no_http]\n",
    "    # Lower case words in doc\n",
    "    tokenized_docs_lowered  = [[word.lower() for word in doc]\n",
    "                                for doc in tokenized_docs]\n",
    "\n",
    "    # Removing punctuation from docs\n",
    "    cleaned_docs = [[remove_symbols(word, punct) for word in doc] \n",
    "                    for doc in tokenized_docs_lowered]\n",
    "\n",
    "    ### Removing stop words\n",
    "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#     docs_no_stops = [[word for word in doc if word not in stop_words] \n",
    "#                      for doc in cleaned_docs]\n",
    "    # Lemmatize words in docs\n",
    "    docs_lemmatized = [[lemmer.lemmatize(word) for word in doc]\n",
    "                      for doc in docs_no_stops]\n",
    "    \n",
    "    # Stem words in docs\n",
    "    docs_stemmed = [[stemmer.stem(word) for word in doc]\n",
    "                      for doc in docs_lemmatized]\n",
    "    \n",
    "    # Removes mentions, emotions, hashtags and emojies\n",
    "    docs_no_mentions = [preprocessing_text(' '.join(doc)) for doc in docs_stemmed]\n",
    "    \n",
    "    bag = []\n",
    "    for doc in docs_no_mentions:\n",
    "        if len(doc) >= 2:\n",
    "            bag.append(doc)\n",
    "    \n",
    "    # converts into list of lists\n",
    "    bow = [list(tweet.split(' ')) for tweet in bag]\n",
    "    \n",
    "    \n",
    "    # convert docs into pd series\n",
    "    full_service_docs_series = pd.Series( (v[0] for v in bow) )\n",
    "    \n",
    "    return bag, bow, docs_stemmed, full_service_docs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venezuela_tweets = pd.read_csv('/Users/alessandro/Downloads/venezuela_201906_1_tweets_csv_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mask = venezuela_tweets['is_retweet'] == False\n",
    "tweets = venezuela_tweets[tweets_mask]\n",
    "\n",
    "tweets_english = tweets[tweets['tweet_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets_english[['user_screen_name','tweet_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(tweet):\n",
    "    tokenized = [ word for word in tweet.split() if not(word.startswith(\"http\")) and not(word.startswith('#'))]\n",
    "    return ' '.join(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bag_of_words'] = data['tweet_text'].apply(get_bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "punct.remove('@')\n",
    "punct.remove('#')\n",
    "\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: x.lower())\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: remove_symbols(x, punct))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', '', x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: lemmer.lemmatize(x))\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Random Forest Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an esemble method. In general, an **ensemble** method combines many weak models to form a strong model. We train multiple models on the data, such that each is different. They could be trained on different subsets of the data, or trained in different ways, or even be completely different types of models.\n",
    "\n",
    "Once we've done that, we need to combine the models to form a single model.\n",
    "\n",
    "Train each learner on different subset of data.\n",
    "\n",
    "A classification tree is a decision tree to predicts whether a data point is in one class or another. Each branch node is a decision, choosing left or right based on the value of a certain feature. Each leaf node gives the probability that a data point is in one class or another.\n",
    "\n",
    "\n",
    "A classification tree is built by\n",
    "\n",
    "1. Iteratively divide the nodes such that (entropy/gini impurity) is minimized\n",
    "2. Various stopping conditions like a depth limit\n",
    "3. Prune trees by merging nodes\n",
    "\n",
    "# Decision Tree Summary\n",
    "What are the pros and cons?\n",
    "\n",
    "Pros\n",
    "\n",
    "1. No feature scaling needed\n",
    "2. Model nonlinear relationships\n",
    "3. Can do both classification and regression\n",
    "4. Robust\n",
    "5. Highly interpretable\n",
    "\n",
    "Cons\n",
    "\n",
    "1. Can be expensive to train\n",
    "2. Often poor predictors because of high variance\n",
    "\n",
    "\n",
    "# Random Forests\n",
    "\n",
    "Bagging decision trees are pretty cool, but the trees still tend to look pretty similar. We want a way to make the trees more different (decorrelate them) without substantially increasing the bias of each tree.\n",
    "\n",
    "Random forests do this with subspace sampling. When we are building a tree and considering the feature to use at each split, we only consider a few of them, randomly chosen. The number of features  𝑚  to consider at each split is a hyperparameter; typically  𝑚=𝑘⎯⎯√  is used.\n",
    "\n",
    "Again, the features to consider are chosen at each split, not each tree. Everyone gets this wrong.\n",
    "\n",
    "Question: are features to consider chosen for each tree, or at each split?\n",
    "\n",
    "For example, suppose we're building a model with nine features. One of them is really predictive, another is pretty good, and the others are just okay.\n",
    "\n",
    "If we build an ensemble of bagged trees, probably each will use the good feature as the first split, and probably each will use the pretty-good feature at the next split. For the other splits the trees might differ, particularly father down when only a few points are being considered, but the first branches will be pretty much the same.\n",
    "\n",
    "If we build a trees in a random forest, we'll only consider three (random) features for that first split. Only a fraction of the trees (around 30%) will consider the good feature on the first split, so they will use that. Some of the others will consider the pretty-good feature, so they will start there. The others will start at some other feature. Those trees will still consider the good and pretty-good features at some of the lower nodes, and will get to take advantage of them, but the overall structure of those trees will be completely different.\n",
    "\n",
    "# Bag of Words\n",
    "Tokenized and preprocessed corpus before vectorization\n",
    "\n",
    "# Vectorization\n",
    "\n",
    "The ultimate goal of indexing is to create a vector representation (signature) for each document. This vector representation will be used for:\n",
    "\n",
    "mine the features that can caracterize classes of documents (supervised learning using labels)\n",
    "mine the documents that have similar features to establish trends (unsupervised learning).\n",
    "To do that, we need:\n",
    "\n",
    "a fixed number of features\n",
    "a quantitative value for each feature.\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The ultimate goal of indexing is to create a vector representation (signature) for each document. This vector representation will be used for: mine the features that can caracterize classes of documents (supervised learning using labels) mine the documents that have similar features to establish trends (unsupervised learning). To do that, we need:\n",
    "\n",
    "a fixed number of features\n",
    "a quantitative value for each feature.\n",
    "The number of features is given by the vocabulary over the corpus: the set of all possible words (tokens) found in all documents.\n",
    "\n",
    "The quantitative value is given, for each doc, by counting the occurences of each of these words in the doc and by using a TF-IDF formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the TFIDF vector\n",
    "Words might show up a lot in individual documents, but their relevace is less important if they're in every document! We need to take into account words that show up everywhere and reduce their relative importance. The document frequency does exactly that:\n",
    "\n",
    "𝑑𝑓(𝑡𝑒𝑟𝑚,𝑐𝑜𝑟𝑝𝑢𝑠)=# 𝑜𝑓 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑠 𝑡ℎ𝑎𝑡 𝑐𝑜𝑛𝑡𝑎𝑖𝑛 𝑎 𝑡𝑒𝑟𝑚# 𝑜𝑓 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑠 𝑖𝑛 𝑡ℎ𝑒 𝑐𝑜𝑟𝑝𝑢𝑠 \n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "𝑖𝑑𝑓(𝑡𝑒𝑟𝑚,𝑐𝑜𝑟𝑝𝑢𝑠)=log1𝑑𝑓(𝑡𝑒𝑟𝑚,𝑐𝑜𝑟𝑝𝑢𝑠) .\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector.\n",
    "\n",
    "tf-idf  =𝑡𝑓(𝑡𝑒𝑟𝑚,𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡)∗𝑖𝑑𝑓(𝑡𝑒𝑟𝑚,𝑐𝑜𝑟𝑝𝑢𝑠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that the TfidfVectorizer() returns floats while the CountVectorizer() returns ints. And that's to be expected – as explained in the documentation quoted above, TfidfVectorizer() assigns a score while CountVectorizer() counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "Meaning assume independence of words\n",
    "\n",
    "probabilities, no distance calculations so efficent for high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Variance vs Bias\n",
    "Boosting\n",
    "\n",
    "1. Lowers variance by growing the model slowly over time (along with a few other tricks).\n",
    "2. Lowers bias by stacking many small models into the final result.\n",
    "\n",
    "We can only calculate residuals at the training data points!\n",
    "\n",
    "We need some way of extending the values of the residuals to places we do not have data.\n",
    "\n",
    "Solution:\n",
    "\n",
    "Fit a model to the residuals!\n",
    "\n",
    "The predictions from this model both:\n",
    "\n",
    "Approximate the residuals at the places as we have data\n",
    "Are defined everywhere\n",
    "\n",
    "## Tuning the Learning Rate\n",
    "The learning rate allows us to grow our boosted model slowly.\n",
    "\n",
    "A large learning rate will cause the model to fit hard to the training data, which creates a high variance situation.\n",
    "\n",
    "A smaller learning rate reduces the boosted models sensitivity to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# # True Positive:\n",
    "1. Interpretation: You predicted positive and it’s true.\n",
    "2. You predicted that a woman is pregnant and she actually is.\n",
    "\n",
    "## True Negative:\n",
    "\n",
    "1. Interpretation: You predicted negative and it’s true.\n",
    "2. You predicted that a man is not pregnant and he actually is not.\n",
    "\n",
    "## False Positive: (Type 1 Error)\n",
    "1. Interpretation: You predicted positive and it’s false.\n",
    "2. You predicted that a man is pregnant but he actually is not.\n",
    "\n",
    "## False Negative: (Type 2 Error)\n",
    "1. Interpretation: You predicted negative and it’s false.\n",
    "2. You predicted that a woman is not pregnant but she actually is.\n",
    "\n",
    "Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "\n",
    "## Recall\n",
    "1. Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "## Precision\n",
    "1. Out of all the classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "## F-measure\n",
    "1. It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Similar users modeled to each other using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsz= data[data.user_screen_name == 'TrumpNewsz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpNewsDaily_= data[data.user_screen_name == 'TrumpNewsDaily_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_news_comparison = pd.concat([TrumpNewsDaily_, TrumpNewsz])\n",
    "trump_news_comparison.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trump_news_comparison.bag_of_words\n",
    "y = trump_news_comparison.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "X_train = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=5,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['TrumpNewsDaily_', 'TrumpNewsz']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=.1,\n",
    "     max_depth=4, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['TrumpNewsDaily_', 'TrumpNewsz']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "Meaning assume independence of words\n",
    "\n",
    "probabilities, no distance calculations so efficent for high dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train.toarray(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test.toarray())\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['TrumpNewsDaily_', 'TrumpNewsz']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=3, random_state=0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['TrumpNewsDaily_', 'TrumpNewsz']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# sgd = Pipeline([('vect', CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "#                 ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "#                ])\n",
    "# sgd.fit(X_train, y_train)\n",
    "\n",
    "# %time\n",
    "\n",
    "# y_pred = sgd.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid Search Parameters\n",
    "# parameters = {\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10,100,300]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.score(X_train, y_train))\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sophia4Trump vs. Sophia Turner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SophiaMillerC = data[data.user_screen_name == 'SophiaMillerC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sophia4Trump = data[data.user_screen_name == 'Sophia4Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophia_comparison = pd.concat([SophiaMillerC, Sophia4Trump])\n",
    "sophia_comparison.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sophia_comparison.bag_of_words\n",
    "y = sophia_comparison.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "X_train = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier on Sophia's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=5,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['SophiaMillerC', 'Sophia4Trump']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Classifier on Sophia's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "     max_depth=4, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['SophiaMillerC', 'Sophia4Trump']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on Sophia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train.toarray(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test.toarray())\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['SophiaMillerC', 'Sophia4Trump']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on sophia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=3, random_state=0, multi_class='multinomial').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['SophiaMillerC', 'Sophia4Trump']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid Search Parameters\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10,100,300]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.score(X_train, y_train))\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# sgd = Pipeline([('vect', CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "#                 ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "#                ])\n",
    "# sgd.fit(X_train, y_train)\n",
    "\n",
    "# %time\n",
    "\n",
    "# y_pred = sgd.predict(X_test)\n",
    "\n",
    "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lauren Jones vs Lauren Jones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaurenJonesGOP = data[data.user_screen_name == 'LaurenJonesGOP_']\n",
    "LJG2 = data[data.user_screen_name == 'LaurenJonesGOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lauren_comparison = pd.concat([LaurenJonesGOP, LJG2])\n",
    "lauren_comparison.user_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lauren_comparison.bag_of_words\n",
    "y = lauren_comparison.user_screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(decode_error='replace', max_features=150)\n",
    "X_train = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier of Lauren Jones's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=5,\n",
    "                             random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have explanation for tfidf, bag of words, vectorizers, random forest, \n",
    "# your result clearly explores what result actually 505. logistic regression, naive bayes.\n",
    "# stay with two and false positive and false negative.\n",
    "# try and change depth, see what results you get as depth gets higher.\n",
    "# read all about the methods random forest gradient boosting. \n",
    "# plot confusion matrix.\n",
    "# good visualization for confusion matrix. a square with fourspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['LaurenJonesGOP', 'LaurenJonesGOP_']\n",
    "cm = confusion_matrix(y_test, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Parameters\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10,100,300]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search on n_estimators, max_depth :10,200,300 and then decrease max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.score(X_train, y_train))\n",
    "# print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
